# Решение 4 кейса ЛЦТ Якутия для Профилум

[Функционал и интерфейс](#Функционал) <br>
[Датасет](#Датасет) <br>
[Модель](#Модель) <br>
[Архитектура](#Архитектура) <br>
[Как развернуть решение](#Деплой) <br><br>
[ОЧЕНЬ ВАЖНО К ПРОЧТЕНИЮ ПЕРЕД ТЕСТИРОВАНИЕМ!](#ВАЖНО) <br><br>

# Функционал и интерфейс
### Список профессий
Проанализировав список профессий, мы пришли к выводу, что предоставленные организатором 50 профессий - хоть и по количеству солидный список, но не охватывает очень многие профессиональные сферы, что может значительно затруднить и запутать выявление профессии для людей, у которых нет склонностей к изначальным 50 профессиям. <br>
Мы решили добавить **еще 50 новых профессий** с уклоном на охват как можно большего количества профессиональных отраслей, не забывая о релевантности профессий. <br>
Для сравнения, вот распределение изначальных профессий (о том, как мы сделали эту картинку - чуть позже):
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/fc16abd5-d279-4c43-931f-c4cb9d1c63af) <br>
А вот распределение всех 100 профессий:
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/57580b13-82eb-4973-a66b-3ed934fbfeee) <br>
Невооруженным взглядом видно, что пропусков между кучными скоплениями изначальных профессий стало гораздо меньше.

### Авторизация и социальные сети
В нашем решении полноценная авторизация реализована через 2 сервиса: Google Account (для YouTube) и VK ID. Можно авторизоваться и через оба сервиса сразу - тогда анализироваться будут обе соц. сети. <br>
Backend сервиса также поддерживает анализ Telegram, однако из веб-интерфейса такую авторизацию прокинуть мы не успели из-за особенностей мессенджера. Анализ доступен только через API напрямую.

### Политика конфидециальности
Решение поддерживает предупреждение пользователя в момент авторизации о том, какие его данные будут собираться, и как сервис планирует ими распоряжаться.

### Дополнительные функции
По окончанию анализа пользователю доступны следующие данные:
* **Хайлайт-профессия**, которая подходит ему больше всего.
* **Список из других профессий**, которые также набрали ощутимый коэффициент по итогам анализа моделью.
* **Для каждой из профессий предоставляется список курсов** - среднего или высшего образования, с указанием университета и названия специальности (курса). В нашей БД, очевидно, нет полных данных о всех курсах в стране, поэтому для некоторых профессий такой список может оказаться пустым. Хорашая новость - этот список весьма удобно пополнять через БД Web-сервиса.
* **Наглядная круговая диаграмма**, показывающая относительное отношение коэффициентов профессий друг к другу.
* **Список из YouTube видео со ссылками**, лайки пользвателя на которых повлияли больше других на итоговый результат (Доступно только при авторизации через Google).
* **Список из YouTube каналов со ссылками**, подписки пользвателя на которые повлияли больше других на итоговый результат (Доступно только при авторизации через Google).
* **Список из VK групп со ссылками**, подписки пользвателя на которые повлияли больше других на итоговый результат (Доступно только при авторизации через VK).

Внутренние фичи:
* **Кэширование API запросов** по паре `id, prefessions_to_value_map` для 3 соц. сетей в отдельных таблицах. Значительно увеличивает скорость отклика ML-сервиса при повторных запросах, экономит квоты на API.
* **API, позволяющее интеграцию backend'a нашего решения** в любое другое решение, а не только в наш веб-сервис.

## Интерфейс
Интерфейс авторизации:
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/b9ae0855-0871-497d-b3b0-999e2195b09d)




# Архитектура
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/086a7e69-fc84-4237-ae66-58a704e8280e)
В нашем решении присутствуют следующие сущности:
* **Web-service**: Веб-сервис с интерфейсом, призванный авторизовывать юзеров и инициировать запросы на анализ соц. сетей пользователя.
* **ML-service**: По своей сути - API, призванное отвечать на запросы анализа данных по соц. сетям. Принимает на вход токены различных соц. сетей, а возвращает набор данных из самых важных поинтов анализа. Отвечает и за парс страничек пользователя.
* **БД**: У нас их две, чтобы максимально изолировать и заложить потенциал для масштабируемости Web и ML сервисов по одиночке. Единственный "мостик" между сервисами - REST API.


# Датасет
Подходя к этой задаче, мы столкнулись со сложной задачей - где найти размеченные данные для обучения модели мультиклассификации? <br>
Нашим решением стало присвоить каждой из профессий набор из "ключевых" слов, которые будут характерны только этой профессии. Вот пример для Digital-Маркетолога: <br>
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/a8409f57-fc50-45f4-9f11-bac833cae075) <br>
Однако, по нашей практике, обычный поиск по ключевым словам в тексте редко дает хорошо размеченные данные. На помощь к нам пришел YouTube API и его умный Search по базе данных с видео, учитывающий не только строгое нахождение предмета поиска в тексте, но еще и ища ключевые слова и их близкие синонимы в тэгах к видео (что работает прекрасно, даже если предмет поиска больше нигде не упоминается), заголовке и описании. Запрашивая через этот интерфейс данные с разными подкомбинациями из ключевых слов для каждой профессии мы смогли разметить около 70000 видео, вдобавок получив для каждого заголовки и описания - данные на которых мы будем обучать модель. <br><br>
Распределение видео по профессиям. Как мы видим, получилась довольно приятная для работы выборка, не требующая сверхусилий для обработки выбросов и нормализации: <br>
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/1fa859a1-6d39-4b95-a913-a420da2df21a) <br>
Вот, для примера, облако из наиболее встречающихся слов для профессии "Кино-оператор": <br>
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/d243dcb1-cbff-435b-a978-f104d6cf298d) <br>


# Модель
В нашем решении присутствует две отдельных модели. <br> 
Первая из них - легковесная и прямолинейная, но показывающая хорошие результаты на нашей выборке - логистическая регрессия с классификатором типа один против всех (OneVsRestClassifier) для обеспечения мульти-лейбльного предсказания по сингл-лейбльно размеченным данным. <br>
Вторая - BERT (DeepPavlov/rubert-base-cased), намного более тяжелая и сложная модель, показывающая несколько более корректные результаты, но намного менее производительная. <br>
Мы решили оставить возможность использовать обе модели (по флагу в cfg), что бы обеспечить большую гибкость нашему решению. <br>
Сравнение моделей: <br>
| Metric        | BERT           | LogReg  |
| ------------- |:-------------:| -----:|
| f1-score      | 0.69 | 0.62 |
| accuracy      | 0.71      |   0.61 |
| disk space | ~700 Mb      |    ~100 Mb |
| time to predict (~150 character text input) | ~4s      |    <0.5s |
| time to train (Tesla T4, 70k videos) | ~5400s      |    ~10s |
<br>

#### Поподробнее о том, как разные данные взаимодействуют с текстовой моделью: <br>
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/a2214c41-c3ae-476e-95e1-0907b1edce43) <br>
Наша модель работает с разными видами источников данных отдельно, даже если они внутри одной соц. сети. В ходе процесса предсказания они мерджатся, иногда - с учётом веса. <br>
Как видно из картинки, модель учитывает следующие данные:
* **YouTube:**
  * Ваши самые "любимые" (`order_by="relevance"` в YT API) блогеры (топ-150) и их последние видео (до 10 штук).
  * Ваши недавние отметки "нравится" на видео (до 100 видео)
* **VK**:
  * Ваши недавние отметки "нравится" под постами (до 100 штук)
  * Ваши собственные посты (до 50 штук)
  * Ваши группы и их последние посты (до 50 штук)
* **Telegram**:
  * Конкатенированные посты из Ваших каналов  


# Деплой
Ниже подробно описывается процесс развертывания данного решения и всего, что для этого пригодится:

## Регистрация и настройка приложения в VK <br>
Приложение в VK нужно для получения веб-сервисом доступа к функционалу авторизации через интерфейс VK ID. <br>
Управлять и создавать приложения можно [отсюда](https://vk.com/apps?act=manage) <br>
Про OAuth авторизацию с помощью VK ID [здесь](https://dev.vk.com/ru/api/access-token/getting-started) <br>
Ваше приложение должно запрашивать доступ к личным данным, а именно - группам, в которых пользователь состоит, его стене и понравившемся записям. <br>

## Регистрация и настройка приложения в Google Cloud API & Services <br>
Приложение Google необходимо для авторизации пользователя через Google аккаунт и для использования YouTube API. <br>
Первым делом нужно создать проект на официальном сайте [Google Cloud API & Services](https://console.cloud.google.com/apis). <br> Затем, в интерфейсе нужно нажать кнопку **+Enable APIs and services** <br><br>
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/9dbcc917-9931-4985-9d03-be35b7dcb55b)
В поиске по библиотеке найти YouTube Data API v3 и активировать данное API для своего проекта ![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/a4668663-3492-4123-97c1-ee7dc87a3a3e)
Затем, во вкладке OAuth consent screen нужно зарегестрировать и настроить Ваше приложение. Из важного на данном этапе - не забыть добавить **auth/youtube.readonly** на шаге **Scopes** и список тестировщиков на последующем шаге. 
**ВАЖНО:** до верификации приложения Google пользоваться им смогут только те пользователи, которые указаны в списке тестировщиков! Поэтому, добавляйте туда всех, кто будет пользоваться сервисом (даже если Вы - владелец проекта)
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/d81b42c6-a399-4f47-a319-bdfed2dbf2ae)
Последний шаг - создание Credentials. Эти данные позволят непосредственно веб-сервису использование API от лица вашего Google Cloud приложения. Для этого заходим во вкладку Credentials и создаем (обязательно!) Credentials для OAuth 2.0 Client IDs
![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/f54ce701-eb99-4d1f-b2ac-c0c1905cd2a6)
Далее скачиваем файл с Credentials в формате JSON и размещаем его по пути ```backend/api_server/secrets/google_project_secret.apps.googleusercontent.com.json``` для корректной работоспособности ML-сервиса. <br>

## Развертка, конфигурация и установка зависимостей для сервисов <br>
На данном этапе будем считать, что все виртуальные машины для подсервисов запущены и развернуты на сервере(ах) в соответствии с [архитектурой](#Архитектура), и на каждой из них есть исходный код. <br><br>
Теперь нужно настроить конфигурации ML и Web сервисов для их корректного общения через REST API, а также для коннекта с базами данных. <br>
**Для ML-сервиса:**
* Создать виртуальное окружение
* Установить зависимости через `pip install -r backend/api_server/requirements.txt`
* ~~Молиться, что все зависимости установились~~
* Настроить файл конфигурации `backend/api_server/cfg.json`
  * `user_bert` - **bool**, флаг для использования модели BERT вместо модели LR.
  * `host` - **string**, адрес хоста при инициализации приложения непосредственно путем запуска `main.py`.
  * `port` - **integer**, порт хоста при инициализации приложения непосредственно путем запуска `main.py`.
  * `db_path` - **string**, "путь" к БД PosgreSQL для кэширования API запросов. Формат: `postgresql://user:password@localhost/dbname`.
* Для использования модели BERT необходимо загрузить её в папку `backend/api_server/models/bert/` под названием `text_model.pth`. Весь каталог `models` (или недостающие части) можно скачать с [Google диска](https://drive.google.com/drive/folders/1rSNaX_uxz2VPm_V4gTETlP-8IBbFgg_w?usp=sharing). В полном развороте каталог должен выглядеть вот так: <br> ![image](https://github.com/DCDsqd/ldt-sakha-solution-2023/assets/89082426/bebbc330-8848-4433-84ae-2afeceb282bc) 


 **Для Web-сервиса:**

## Запуск сервисов <br>
**Для ML-сервиса**: Из папки ```backend/api_server``` запустите ```python main.py```, чтобы запустить сервис с конфигурацией из конфига, либо можно указать `host` и `port` напрямую, выполнив `uvicorn main:app --host [host] --port [port]`.



# ВАЖНО
К сожалению, VK и Google в целях безопасности не позволяют непроверенным приложениям получать доступ ко многим данным пользователя. Поэтому, до верификации этих приложений VK API работать не будет вообще, а Google будет работать только для тех пользователей, которые числятся как тестировщики (см. **Регистрация и настройка приложения в Google Cloud API & Services**, либо попросите нас добавить нужные вам аккаунты как тестировщиров) <br>
Другого обходного легального пути для получения данных из этих сервисов нет. <br>
### **Приложения, через которые работает сервис, запущенный на хосте для демонстрации на момент дедлайна НЕ ВЕРИФИЦИРОВАНЫ -> функционал серъезно ограничен!** <br>
### **Если у вас не работает авторизация или некоторые запросы - дело в этом!** <br>
Мы оставили заявки на верификацию наших приложений, но так и не дождались её :(

**Ещё одно важное замечание по поводу квоты YouTube API:** Для неверефицированного приложения квота - 10000 токенов в день. На обслуживание одного пользователя за счёт наших внутренних оптимизаций уходит примерно 200-250 токенов, однако этого все равно может не хватить. Можно зарегестрировать несколько приложений (вроде как это не противоречит правилам Google), однако автоматического способа это делать у нас нет. После верификации приложения квота значительно увеличивается. К тому же, можно запросить бесплатное повышение квоты, подробно описав свою задачу и планы на использование в заявке. <br><br>

Спасибо за внимание к нашему решению!
