{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Параметры\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "max_length = 200  # Максимальная длина текста\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device.type)\n",
    "\n",
    "# Загрузка токенизатора и модели\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=100)\n",
    "model = model.to(device)\n",
    "\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Функции для предобработки текста\n",
    "def clean_text_from_unnecessary_symbols(text):\n",
    "    text = re.sub(r'[^а-яА-Яa-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return ' '.join([word for word in words if word.lower() not in russian_stopwords and word.lower() not in english_stopwords])\n",
    "\n",
    "def clean_text_for_model(text):\n",
    "    text = remove_stop_words(text)\n",
    "    text = remove_urls(text)\n",
    "    text = clean_text_from_unnecessary_symbols(text)\n",
    "    return text\n",
    "\n",
    "# Загрузка и предобработка данных\n",
    "conn = sqlite3.connect('yt_videos_for_train_labeled.db')\n",
    "videos_df = pd.read_sql('SELECT * FROM videos', conn)\n",
    "videos_df['text'] = videos_df['video_title'].astype(str) + \" \" + videos_df['video_description'].astype(str)\n",
    "videos_df['text'] = videos_df['text'].apply(clean_text_for_model)\n",
    "\n",
    "X = videos_df['text']\n",
    "y = videos_df['profession']\n",
    "\n",
    "# Разделение данных и кодирование меток\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)\n",
    "\n",
    "# Токенизация текстов\n",
    "def encode_texts(texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "train_inputs, train_masks = encode_texts(train_texts)\n",
    "val_inputs, val_masks = encode_texts(val_texts)\n",
    "\n",
    "# Создание DataLoader'ов\n",
    "train_data = TensorDataset(train_inputs, train_masks, torch.tensor(train_labels))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(val_inputs, val_masks, torch.tensor(val_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# Оптимизатор и планировщик\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Функция обучения модели\n",
    "def train(model, train_dataloader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            b_input_ids = b_input_ids.to(device)\n",
    "            b_input_mask = b_input_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "# Функция оценки модели\n",
    "def evaluate(model, validation_dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            b_input_ids = b_input_ids.to(device)\n",
    "            b_input_mask = b_input_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            predictions.extend(np.argmax(logits, axis=1).flatten())\n",
    "            true_labels.extend(label_ids.flatten())\n",
    "\n",
    "    return accuracy_score(true_labels, predictions), classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
    "\n",
    "# Обучение и оценка модели\n",
    "train(model, train_dataloader)\n",
    "accuracy, report = evaluate(model, validation_dataloader)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(report)\n",
    "\n",
    "# Сохранение модели и токенизатора\n",
    "model_path = 'text_model.pth'\n",
    "tokenizer_path = 'tokenizer/'\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "if not os.path.exists(tokenizer_path):\n",
    "    os.makedirs(tokenizer_path)\n",
    "tokenizer.save_pretrained(tokenizer_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
